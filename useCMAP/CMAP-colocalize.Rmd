---
title: <center> <h1>Obtaining covariates from cruise trajectory via R </h1> </center>  
## author: Sangwon Hyun, Aditya Misha, Jacob Bien, Christian Mueller
output: html_document
---
```{r global_options, include=FALSE}
knitr::opts_chunk$set(fig.width=14, fig.height=5, echo=TRUE, warning=FALSE,
                      message=FALSE, eval=TRUE, cache=TRUE)
library(popcycle)
library(odbc)
library(tidyverse)
outputdir = "~/Desktop"
source = "~/Dropbox/research/usc/flow-cytometry/data/MGL1704.csv"
source("~/Dropbox/CMAP-Access/getAttributeFun.R")
source("~/Dropbox/CMAP-Access/colocalize.R")
```

This document explains how to use R to get colocalized environmental data for a
cruise named "MGL1704" provided by Francois Ribalet with the Ambrust lab. Some
of the material comes directly from here:
[https://github.com/armbrustlab/popcycle/wiki/SeaFlow-data-analysis-tutorial](wiki).

# Getting the cruise trajectory (skim)
From the MGL1704 cruise, we can load a table from the DB file
"MGL1704/MGL1704.db" that contains cruise-level data:

```{r setup}
## RETRIEVE CRUISE-LEVEL DATA
path <- "~/Desktop/" 
cruise <- c("MGL1704") 
db <- paste0(path, cruise, "/", cruise, ".db")
sfl <- get.sfl.table(db) 
sfl <-  sfl[-nrow(sfl),]
head(sfl) 

## Get all the cytograms that are "robust"
out = get.outlier.table(db)

## Match the dates,
dates = sapply(out[,1],function(a){
  dt = substr(a,10,19)
  tm = substr(a,21,100)
  tm = str_replace_all(tm, "-", ":")
  paste0(dt, "T", tm)
})

## Isolate our attention to only robust /dates/, according to the names of the images.
all.robust = which(out$flag==0)#
sfl = sfl[-all.robust,]

## There is also one NA value, don't know why.
npoints = nrow(sfl)  ## npoitns=100
sfl = sfl[-which(is.na(sfl[1:npoints,"lon"])),]
```

From this, we create a CSV file containing a table which contains three data
columns 'lat', 'lon' and 'date'. We will use these as target triplets in space
and time, for colocalization.

```{r make-csv}
csvtable = sfl[,c("date", "lat", "lon")]
csvfilename = "MGL1704.csv"

## convert date to cmap's desired format.
dates = csvtable[,1]
newdates = sapply(dates, function(mydate){
  dt = substr(mydate, 1,10)
  tm = substr(mydate, 12,19)
  paste0(dt, "T", tm)
})
csvtable[,1] = newdates
names(csvtable)[1] = "time"

## Date
write.csv(csvtable, file=file.path(outputdir, csvfilename), row.names=FALSE)
head(csvtable)
dim(csvtable)
```

This CSV table can be replaced with a manually created table of the same format,
or a "virtual cruise" obtained through the Simons CMAP GUI program.

# Taking a hint from python for writing the R functionality
The python script to colocalize is as follows:

```{python, eval=FALSE}
from opedia import colocalize as COL

DB = False                            # < True > if source data exists in the database. < 0 > if the source data set is a spreadsheet file on disk.
source = '~/Desktop/MGL1704.csv'            # the source table name (or full filename)
temporalTolerance = 1                # colocalizer temporal tolerance (+/- days)
latTolerance = 0.3                   # colocalizer meridional tolerance (+/- degrees)
lonTolerance = 0.3                   # colocalizer zonal tolerance (+/- degrees)
depthTolerance = 5                   # colocalizer depth tolerance (+/- meters)
tables = ['tblSST_AVHRR_OI_NRT', 'tblPisces_NRT', 'tblDarwin_Plankton_Climatology']    # list of varaible table names


variables = ['sst', 'Fe', 'picoeukaryote_c03_darwin_clim']                            # list of variable names
exportPath = './loaded.csv'         # path to save the colocalized data set

COL.matchSource(DB, source, temporalTolerance, latTolerance, lonTolerance, depthTolerance, tables, variables, exportPath)
```

Put simply, the python function `COL.matchsource()` takes in

1. A CSV file containing date/lon/lat (e.g. "MGL1704.csv").
2. A set of "tolerance" variables for colocalization (e.g. temporalTolerance).
3. Tables (data sources) to look into.
4. Variables within those tables.
5. Output file name (e.g. "./loaded.csv").

Then, it loops over the tables and variables, and for each triplet
(lat/lon/time), it issues individual queries to obtain a sub-dataset
correposponding to the "rectangle", or vicinity of the target triplet. From each
sub-dataset, it calculates the mean and standard deviation.

We'll replicate this in our R implementation.

# The R function

The R function `colocalize` looks like this (in `./colocalize.R`):

```{r}
##' Mimics the matchSource() function in python.
##' @param table.name Table name
##' @param sel.vars A vector  of variable  names in this  table that are  to be
##'   queried
##' @param latMargin Latitude margin.
##' @param lonMargin Longitude margin.
##' @param timeMargin Time margin.
##' @param source The  file name  of the  csv source  file that  contains the
##'   lat/lon/date triplets.
matchSource_onetable <- function(con, source, table.name, sel.vars,
                        latMargin, lonMargin, timeMargin,
                        orderby=NULL
                        ){

  ## Read in source table
  source.table = read.csv(source)
  ntriplets = nrow(source.table)

  ## Empty data frame to fill in
  DF = data.frame(matrix(nrow=ntriplets)[,-1])

  DF[["lat"]] = source.table[,"lat"]
  DF[["lon"]] = source.table[,"lon"]
  DF[["time"]] = source.table[,"time"]


  ## TODO Remember to add depth query
  ## if not 'depth' in srcDF.columns:
  ##     srcDF['depth'] = 0

  ## For each variable in "variables",
  ## TODO: loop over tables as well, or wrap around this.
  for(sel.var in sel.vars){
    means = rep(NA, ntriplets)
    stds = rep(NA, ntriplets)
    for(itriplet in 1:ntriplets){
      printprogress(itriplet,ntriplets)
      lat = source.table[itriplet,"lat"]
      lon = source.table[itriplet,"lon"]
      dt = source.table[itriplet, "time"]

      ## Query that subtable:
      range.var = return_range(lat, lon, dt, latMargin, lonMargin, timeMargin)
      tbl.subset = getTableData(con, table.name, sel.var, range.var, orderby)

      ## Aggregate it (for now, only calculating means)
      if(nrow(tbl.subset)==0){
        means[itriplet] = NA
      } else {
        values = tbl.subset[[sel.var]]
        means[itriplet] = mean(values, na.rm=TRUE) ## Removing the NAs
        stds[itriplet] = sd(values, na.rm=TRUE) ## Removing the NAs
      }
    }
    DF[[sel.var]] = means
    DF[[paste0(sel.var, "-std")]] = stds
  }
  return(DF)
}

```

# Some specific steps required (skip, not important)

Here are the specifics. First, take the data catalog downloaded from
"catalog.py" per directions here
https://cmap.readthedocs.io/en/latest/catalog/catalog.html. Assume that has been
done and "catalog.csv" exists. We'll isolate our attention to just the table
names and variable names therein.

```{r, eval=FALSE, echo=TRUE}
## Read tablenames and variablenames from the catalog
filedir = "/home/shyun/Dropbox/research/usc/flow-cytometry/data"
filename = "catalog.csv"
tab = read.csv(file.path(filedir, filename))
tab = tab[,c("Variable", "Table_Name")]
tab = as.matrix(tab)

## Define table names and variable names for further use
tablenames = sapply(unique(tab[,"Table_Name"]), toString)
variablenames = lapply(tablenames, function(tablename){
  sapply(tab[which(tab[,"Table_Name"]==tablename), "Variable"], toString)
})
names(variablenames) = tablenames
```

From this two-column catalog, we have the (table name -- variable name) pairs in
each row. Now, we will find which tables have lat/lon/time overlap with our
desired triplets ("source"), to isolate our attention to fewer tables. We then
further write these into a csv file.

```{r overlap, echo=FALSE, eval=FALSE}
## Get all the lat/lon/date ranges of all tables
source("~/Dropbox/CMAP-Access/getAttributeFun.R")
con = DBI::dbConnect(odbc::odbc(), DSN="CMAP-MSSQL",UID="ArmLab",PWD="ArmLab2018")
tablenames = sapply(unique(tab[,"Table_Name"]), toString) 
start.time = Sys.time()
outputdir = "~/Dropbox/research/usc/flow-cytometry/data/table-ranges"
for(tablename in tablenames[9:length(tablenames)]){
  print(Sys.time())
  print(tablename)
  tryCatch({
    tibble.table = getSpaceTimeRange(con, tablename)
  }, error=function(e){
    cat("ERROR :", conditionMessage(e), "\n")
  })
  print(tibble.table)
  save(tibble.table, file=file.path(outputdir, paste0("range-tibble-", tablename, ".Rdata")))
}

## Load ranges from the tibble tables, and transform /each/.
source("~/Dropbox/research/usc/flow-cytometry/data/covariates-helpers.R")
valid.tablenames = tablenames[-c(8,12,13,14,33)]
range.tables = lapply(valid.tablenames, function(tablename){
  print(tablename)
  load(file=file.path(outputdir, paste0("range-tibble-", tablename, ".Rdata")))
  range_tibble_to_table(tibble.table)
})
names(range.tables) = valid.tablenames
save(range.tables, file=file.path(outputdir, "tables-range.Rdata"))
```

```{r, echo=TRUE}
## Take each table's range, compare with source's range.
load(file=file.path("./", "tables-range.Rdata"))
relevant.tablenames = match.ranges(range.tables, source)
elevant.tablenames = relevant.tablenames[-5] ## Excluding the simulated phytoplankton species
print(relevant.tablenames)
```

# Example of results

Now, run the colocalization on all relevant tables whose names are in
`relevant.tablenames`, as follows:

```{r,eval=FALSE, echo=TRUE}
## Setup
con = DBI::dbConnect(odbc::odbc(), DSN="CMAP-MSSQL",UID="ArmLab",PWD="ArmLab2018")
source = './MGL1704.csv'
latMargin = 0.3
lonMargin = 0.3
timeMargin = 1

## For each relevant table, query the variables, and see if /anything/ comes out
## of it.
for(ii in 1:length(relevant.tablenames)){
  tablename = relevant.tablenames[ii]
  res = matchSource_onetable(con, source, tablename, variablenames[[tablename]],
                             latMargin, lonMargin,
                             timeMargin)
  save(res, file=file.path(outputdir,
                           paste0("res-", tablename, ".Rdata")))
}
```


The results look like this, for a given table and all variables therein (in this case, only one).
```{r show-results, eval=TRUE, echo=TRUE}
tablename = "tblSST_AVHRR_OI_NRT"
ichunk = 1
load(file=file.path(outputdir,
                    paste0("res-", tablename, "-chunk-", ichunk, ".Rdata")))
print(head(res[,-5], 10))
plot(res[500:1000,4], type='o', lwd=2, col='violet')
plot(res[500:1000,1:2], type='l', lwd=2, col='black')
```
 
```{r, echo=TRUE, eval=FALSE}
## Setup
source = './MGL1704.csv'
latMargin = 0.3
lonMargin = 0.3
timeMargin = 1
tablename = "tblSST_AVHRR_OI_NRT"
variablename = "sst"

## For the desired table, query a variable therein
res = matchSource_onetable(con, source, tablename, variablename,
                           latMargin, lonMargin, timeMargin)
```
